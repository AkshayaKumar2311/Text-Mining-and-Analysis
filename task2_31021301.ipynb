{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1- FIT5196 Data Wrangling\n",
    "\n",
    "\n",
    "# Task 2: Text Pre-Processing\n",
    "\n",
    "\n",
    "## Name: Akshaya Kumar Chandrasekaran         \n",
    "\n",
    "\n",
    "## ID : 31021301\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, an excel file with multiple sheets related to COVID-19 is given. \n",
    "\n",
    "Contents begin at different places in each sheet. \n",
    "\n",
    "### **Given:**\n",
    "\n",
    "An excel document with mutiple sheets(each sheet is a day's tweet)\n",
    "\n",
    "### **Asked:**\n",
    "\n",
    "+ To generate top 100 Unigrams per day\n",
    "\n",
    "+ To generate top 100 bi-grams per day\n",
    "\n",
    "+ To create an Vocab text after removing the context dependent stopwords, context independent stop words, words that are less than 3 in length. \n",
    "\n",
    "    + Context Dependent Stop words Criteria : Words that occur in more than 60 days\n",
    "    \n",
    "    + Rare token : Words that occur in less than 5 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Methodology to solve the problem\n",
    "\n",
    "\n",
    "## 2.1 Importing packages:\n",
    "\n",
    "Importing the allowed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SAIC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import langid\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.probability import *\n",
    "from itertools import chain\n",
    "from nltk.util import ngrams\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reading the files\n",
    "\n",
    "The given excel file is read in python and is being stored. To complete this task, pandas package is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the given excel file \n",
    "given_excel = pd.ExcelFile(r'31021301.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Importing context dependent stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the stop words text file in read only mode\n",
    "file_stop = open(\"stopwords_en.txt\",\"r\")\n",
    "\n",
    "#reading each line and storing it as a list.\n",
    "stop_words_list = file_stop.read().splitlines()\n",
    "\n",
    "#Closing the file\n",
    "file_stop.close()\n",
    "\n",
    "#Converting the list to set and then back to list to avoid duplicates\n",
    "stop_words = list(set(stop_words_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Defining and declaring regex tokenizer to tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring the regular expression to tokenize the words\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Defining user defined functions:\n",
    "\n",
    "In this section, few user defined functions are defined to create top unigrams per day, top bigrams per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_unitoken_perday() is an user defined function that takes in a list as argument. \n",
    "This function returns two lists, \n",
    "unigrams_english_only --> Consists of only english tweets\n",
    "token_day_unigram --> list of tokenized words per day\n",
    "'''\n",
    "\n",
    "#User defined function\n",
    "def generate_unitoken_perday(only_text):\n",
    "    \n",
    "    #Declaring variables to store the values as list\n",
    "    unigrams_english_only = []\n",
    "    token_day_unigram = []\n",
    "    \n",
    "    #Iterating throught the range of tweets\n",
    "    for i in range(len(only_text)):\n",
    "        \n",
    "        #Checking if the type is only string and if the tweet language is in english\n",
    "        if((type(only_text[i]) == str) and langid.classify(only_text[i])[0]=='en'):\n",
    "            \n",
    "            #Encoding and decoding is done for emoji handling\n",
    "            temp = only_text[i].encode('ascii','ignore').decode('unicode-escape')\n",
    "            only_text[i] = temp.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "            \n",
    "            #appending it to a list if the above conditions are met\n",
    "            unigrams_english_only.append(only_text[i])\n",
    "            \n",
    "    #From the list got above, for each sentense applying the tokeniser defined above to extract only\n",
    "    #required and relevant words.\n",
    "    for j in range(len(unigrams_english_only)):\n",
    "        \n",
    "        #Applying on each sentense and extracting a list of words\n",
    "        unigram_tokens_words = tokenizer.tokenize(unigrams_english_only[j].lower())\n",
    "        \n",
    "        #for the length of the list of words got above\n",
    "        for k in range(len(unigram_tokens_words)):\n",
    "            \n",
    "            #checking if the length of the list is greater than 3 and \n",
    "            #if the words are not in stop words\n",
    "            if(len(unigram_tokens_words[k]) >=3 and (unigram_tokens_words[k] not in stop_words)):\n",
    "                \n",
    "                #Converting it to lower and then appending it to perday list\n",
    "                token_day_unigram.append(unigram_tokens_words[k])\n",
    "                \n",
    "    #Returing values to function call\n",
    "    return unigrams_english_only,token_day_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_bitoken_perday() is an user defined function that takes in only english tweets as inputs\n",
    "and from that, apply tokenization and extract only the required and needed words. No removal of stop words or\n",
    "words of length less than 3 is done as per the sample output given.\n",
    "\n",
    "This method returns a list of words after tokenisation\n",
    "'''\n",
    "\n",
    "#user defined function\n",
    "def generate_bitoken_perday(eng_only_uni):\n",
    "    \n",
    "    #Declaring variable to store the values as list\n",
    "    token_day_bigram = []\n",
    "    \n",
    "    #Iterating throught the range of english tweets\n",
    "    for j in range(len(eng_only_uni)):\n",
    "        \n",
    "        #Applying on each sentense and extracting a list of words\n",
    "        unigram_tokens_words = tokenizer.tokenize(eng_only_uni[j])\n",
    "        \n",
    "        #for the length of the list of words got above\n",
    "        for k in range(len(unigram_tokens_words)):\n",
    "            \n",
    "            #Converting it to lower and then appending it to perday list\n",
    "            token_day_bigram.append(unigram_tokens_words[k].lower())\n",
    "   \n",
    "    #Returing values to function call\n",
    "    return token_day_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_unigram() is a user defined function that takes in the stemmed vocabulary and the current date as input. \n",
    "\n",
    "Based on the inputs, it will generate top 100 unigrams for the day and then write it to the text file.\n",
    "'''\n",
    "\n",
    "#User defined function\n",
    "def generate_unigram(vocab_stemmed,curr_date):\n",
    "    \n",
    "    #using the function FreqDist to find each vocab's frequency\n",
    "    unigram_curr_day = FreqDist(vocab_stemmed)\n",
    "    \n",
    "    #initialising a dict to store the values in key value pair\n",
    "    uni_dic = {}\n",
    "    \n",
    "    #Based on the frequency from the above FreqDist fucntion, sorting it in reverse order\n",
    "    #to get the top 100 unigrams for the day\n",
    "    sorted_unigram_curr_day = sorted(unigram_curr_day.items(),key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    #openinng the file\n",
    "    uni_word_file = open('31021301_100uni.txt','a')\n",
    "    \n",
    "    #openinng the file\n",
    "    #Checking if the length of the list is greater than 100\n",
    "    check_count = len(sorted_unigram_curr_day)\n",
    "    \n",
    "    \n",
    "    #openinng the file\n",
    "    #Checking if the length of the list is greater than 100\n",
    "    #if the length is greater than 100, print only the top 100 words\n",
    "    if(check_count >100):\n",
    "        uni_dic[curr_date] = sorted_unigram_curr_day[0:100]\n",
    "        \n",
    "        #Writing each word and its corresponding frequency into the file\n",
    "        for word, count in uni_dic.items(): \n",
    "            uni_word_file.write('%s:%s\\n' % (word, count))\n",
    "    \n",
    "    #IF the length of the list is less than 100, print entire list\n",
    "    else:\n",
    "        uni_dic[curr_date] = sorted_unigram_curr_day\n",
    "        \n",
    "        #Writing each word and its corresponding frequency into the file\n",
    "        for word, count in uni_dic.items(): \n",
    "            uni_word_file.write('%s:%s\\n' % (word, count))\n",
    "    \n",
    "    #closing the file\n",
    "    uni_word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_bigram() is a user defined function that takes in the vocabulary and the current date as input. \n",
    "\n",
    "Based on the inputs, it will generate top 100 bigrams for the day and then write it to the text file.\n",
    "'''\n",
    "#User defined function\n",
    "def generate_bigram(vocab_stemmed,curr_date):\n",
    "    \n",
    "    #Using the inbuilt package and collocation funtion to create most frequent 100 bigrams for the day\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    \n",
    "    #passing the token words\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(vocab_stemmed)\n",
    "    \n",
    "    #Finding the top 100 bigrams\n",
    "    bigram_dict = list(bigram_finder.ngram_fd.most_common(100))\n",
    "    \n",
    "    #a varibale to store is as key value pair\n",
    "    bi_dic ={}\n",
    "    \n",
    "    #openinng the file\n",
    "    bi_word_file = open('31021301_100bi.txt','a')\n",
    "    \n",
    "    \n",
    "    #Checking if the length of the list is greater than 100\n",
    "    check_count = len(bigram_dict)\n",
    "    \n",
    "    #if the length is greater than 100, print only the top 100 words\n",
    "    if( check_count>100):\n",
    "        \n",
    "        bi_dic[curr_date] = bigram_dict[0:100]\n",
    "        \n",
    "        #Writing each word and its corresponding frequency into the file\n",
    "        for word, count in bi_dic.items(): \n",
    "            bi_word_file.write('%s:%s\\n' % (word, count))\n",
    "    \n",
    "    #IF the length of the list is less than 100, print entire list\n",
    "    else:\n",
    "        bi_dic[curr_date] = bigram_dict\n",
    "        \n",
    "        #Writing each word and its corresponding frequency into the file\n",
    "        for word, count in bi_dic.items(): \n",
    "            bi_word_file.write('%s:%s\\n' % (word, count))\n",
    "    \n",
    "    #File close\n",
    "    bi_word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_raw is an user defined function that takes in list of all english tweets as input and tokenize it\n",
    "and returns a list of tokenized words.\n",
    "'''\n",
    "\n",
    "#user defined function\n",
    "def generate_raw_tokens(sep_tweets):\n",
    "    \n",
    "    #Empty list to append and store the tokenized words\n",
    "    token_words = []\n",
    "    \n",
    "    #Iterating through each sentense(each tweet)\n",
    "    for i in range(len(sep_tweets)):\n",
    "        \n",
    "        #Tokenizing it with the regular expression provided\n",
    "        unigram_tokens_words = tokenizer.tokenize(sep_tweets[i])\n",
    "        \n",
    "        #For the length of each token\n",
    "        for j in range(len(unigram_tokens_words)):\n",
    "            \n",
    "            #Appending it to the token_words in lower case\n",
    "            token_words.append(unigram_tokens_words[j].lower())\n",
    "            \n",
    "    #returning the list\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_top200_bigrams is an user defined function that takes in two arguments\n",
    "token_words --> A list of tokenized words\n",
    "day_uni_words --> A dictionary with key as date and value as tokens in the corresponding date\n",
    "\n",
    "This function will generate top 200 bi-grams.\n",
    "BigramAssocMeasures --> From this method, pointwise mutual information (pmi) will give us \n",
    "the top n bigrams given inside the argument \n",
    "\n",
    "BigramCollocationFinder --> Find the bigrams from the words passed in arguments\n",
    "\n",
    "MWETokenizer is applied to retokenize the tokens\n",
    "\n",
    "This function returns the list of unique tokens\n",
    "'''\n",
    "\n",
    "#User defined function\n",
    "def generate_top200_bigrams(token_words,day_uni_words):\n",
    "    \n",
    "    #For finding the top PMI\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    \n",
    "    #Collocations on the done on the token words\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(token_words)\n",
    "    \n",
    "    #top 200 bigrams are found\n",
    "    bigram_200 = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "    \n",
    "    #Tokenises the tokenised text\n",
    "    mwetokenizer = MWETokenizer(bigram_200)\n",
    "    \n",
    "    #creates a dictionary (key value pair) with \n",
    "    #key as the date and value as the individual tokens in the date\n",
    "    colloc_patents =  dict((pid, mwetokenizer.tokenize(patent)) for pid,patent in day_uni_words.items())\n",
    "    \n",
    "    all_words_colloc = list(chain.from_iterable(colloc_patents.values()))\n",
    "    \n",
    "    #Converting to set and then back to list to avoid duplicate tokens\n",
    "    colloc_voc = list(set(all_words_colloc))\n",
    "    \n",
    "    #returning the list of unique tokens\n",
    "    return colloc_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find_context_dependent_words is an user defined function that takes in one list as its argument \n",
    "convert to set to remove duplicates and then back to list for iteration purpose. \n",
    "\n",
    "If the words count is greater than 60 or if the words count is less than 5, then words are appended to the list \n",
    "\n",
    "This list containing all context dependent words is then returned\n",
    "'''\n",
    "\n",
    "#User defined function\n",
    "def find_context_dependent_words(temp_list):\n",
    "    \n",
    "    #to avoid duplicates\n",
    "    unique_words = list(set(temp_list))\n",
    "    \n",
    "    #Creating a new list to store the context dependent words\n",
    "    context_dependent = []\n",
    "    \n",
    "    #Iterating through the list to compare and add the context dependent words\n",
    "    for i in range(len(unique_words)):\n",
    "        \n",
    "        #if the count of the words are greater than 60 or less than 5(for rare tokens)\n",
    "        if(temp_list.count(unique_words[i]) > 60 or temp_list.count(unique_words[i]) < 5):\n",
    "            \n",
    "            #appending it to the list\n",
    "            context_dependent.append(unique_words[i])\n",
    "    \n",
    "    #Returning the list of context dependent words\n",
    "    return context_dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "remove_unwanted_words is a user defined function that takes in three lists as arguments\n",
    "colloc_voc --> list of collocated wordS( actual tokens)\n",
    "context_dependent--> context dependent stop words\n",
    "stop_words--> context independent stop words\n",
    "\n",
    "All three has been converted to set to that set operations can be performed.\n",
    "\n",
    "Since we have to remove the context dependent words from the colloc_voc, just a minus would perform the\n",
    "action much faster. Like wise, operations has been performed and \n",
    "\n",
    "1. Context dependent\n",
    "2. Context independent \n",
    "3. words of length less than 3 has been removed.\n",
    "'''\n",
    "\n",
    "#user defined function\n",
    "def remove_unwanted_words(colloc_voc,context_dependent,stop_words):\n",
    "    \n",
    "    #converting to set\n",
    "    set_coll = set(colloc_voc)\n",
    "    \n",
    "    #converting to set\n",
    "    set_cont = set(context_dependent)\n",
    "    \n",
    "    #converting to set\n",
    "    set_stop = set(stop_words)\n",
    "    \n",
    "    #Subtracting collocated from the context dependent. \n",
    "    #what is there in collocated set and not in context dependent set\n",
    "    temp_1 = set_coll - set_cont\n",
    "    \n",
    "    #there in (set_coll - set_cont) and not in stop words\n",
    "    temp_2 = temp_1 - set_stop\n",
    "    \n",
    "    #Conveting to back to list for iteration purpose\n",
    "    temp_2 = list(temp_2)\n",
    "    \n",
    "    #if the length of the tokens are greater than or equal to 3, include them\n",
    "    proper_tokens = [i for i in temp_2 if len(i)>=3]\n",
    "    \n",
    "    #return the proper tokens generated after \n",
    "    return proper_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_vocab is an user defined function that generates the vocabulary text file. Takes in the stemmed words \n",
    "as input\n",
    "'''\n",
    "\n",
    "#user defined function\n",
    "def generate_vocab(vocab_stemmed):\n",
    "    \n",
    "    #initialising count to 0 for indexing purpose\n",
    "    count=0\n",
    "    \n",
    "    #opening the file\n",
    "    file = open('31021301_vocab.txt','w',encoding = 'utf-8')  \n",
    "    \n",
    "    #For the range of length of the text\n",
    "    for i in vocab_stemmed:\n",
    "        \n",
    "        #write te word with it's count\n",
    "        file.write(i + ':'+ str(count) + '\\n')\n",
    "        \n",
    "        #increamenting the count\n",
    "        count+=1\n",
    "        \n",
    "    #closing the file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_countVec is a user defined function that takes in a dictionary as input.\n",
    "\n",
    "For each sheet, set of tokens is taken and then updated in the vocab dict. \n",
    "tokens are appended to the vocab list to perform the count operation to get the context dependent\n",
    "stop words.\n",
    "Making sure than _ is not there in the text(this is for bigrams seperated with an _)\n",
    "\n",
    "Once done, context independent stop words, context dependent stop words, words in length less than 3 and bigrams are \n",
    "removed and then stemmed if it is not a bigram, else appending it directly.\n",
    "\n",
    "This function will return the dictionary with key as its date and value as its corresponding tokens reqd.\n",
    "'''\n",
    "\n",
    "#user defined function\n",
    "def generate_countVec(day_uni_words):\n",
    "    \n",
    "    #dummy variable to extract only the tokens \n",
    "    vocab_dict = {}\n",
    "\n",
    "    #For the range of the lenght of keys\n",
    "    for i in range(len(day_uni_words)):\n",
    "        \n",
    "        #set to avoid duplicates\n",
    "        unique_tokens = set(day_uni_words[given_excel.sheet_names[i]])\n",
    "        \n",
    "        #with the sheet name as key and the unique tokens as value\n",
    "        #updating it to the dictionary\n",
    "        vocab_dict.update({given_excel.sheet_names[i]:unique_tokens})\n",
    "    \n",
    "    #Extracting onlt the values and storing it in list\n",
    "    vocab_list = [list(value) for value in vocab_dict.values()]\n",
    "    \n",
    "    #to create a single list from list of lists\n",
    "    vocab_list = sum(vocab_list,[])\n",
    "    \n",
    "    #ingoring the _(bigram words) and adding only uni tokens\n",
    "    vocab_list = [a for a in vocab_list if \"_\" not in a]\n",
    "    \n",
    "    #counting frequency of words to get to know the context dependent words\n",
    "    freq_vocab_list = FreqDist(vocab_list)\n",
    "    \n",
    "    #appending the context dependent words to the variable\n",
    "    context_dep_vec = [key for key,value in freq_vocab_list.items() if value > 60 or value > 5]\n",
    "    \n",
    "    #Converting it to set and then back to list to avoid duplicates\n",
    "    context_dep_vec = list(set(context_dep_vec))\n",
    "    \n",
    "    #Keeiping only the required tokens (i,e) removing dependent words, independent words,\n",
    "    #words less than length 2, stemming the words\n",
    "    for i in day_uni_words.keys():\n",
    "        day_uni_words[i] = [x for x in day_uni_words[i] if x not in context_dep_vec]\n",
    "        day_uni_words[i] = [x for x in day_uni_words[i] if len(x) >= 3]\n",
    "        day_uni_words[i] = [x for x in day_uni_words[i] if x not in stop_words]\n",
    "        day_uni_words[i] = [stemmer.stem(x) if \"_\" not in x else x for x in day_uni_words[i]]\n",
    "    \n",
    "    #returning the dictionary\n",
    "    return day_uni_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_vecdoc is an user defined function that takes in 3 arguments\n",
    "day_uni_words_final --> Final dictionary received from the above function\n",
    "given_excel --> Excel sheet to iterate through names of the sheet\n",
    "vocab_stemmed --> vocab stemmed to check if the word is present in the vocab file\n",
    "'''\n",
    "\n",
    "#user defined function\n",
    "def generate_vecdoc(day_uni_words_final,given_excel,vocab_stemmed):\n",
    "    \n",
    "    #Initialising the count vectorizer\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "    \n",
    "    #Performing fit and transformation of the vectorizer on the tokens in the dictionary file\n",
    "    data_features = vectorizer.fit_transform([' '.join(value) for value in day_uni_words_final.values()])\n",
    "    \n",
    "    #Getting the feature names\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    #opening the document\n",
    "    count_vec_file = open('31021301_countVec.txt','a')\n",
    "        \n",
    "        #for the range of length of the excel sheets\n",
    "        for i in range(len(given_excel.sheet_names)):\n",
    "            \n",
    "            #append the sheet name\n",
    "            count_vec_file.write(given_excel.sheet_names[i] + \",\")\n",
    "            \n",
    "            #Zipping the vocab and count \n",
    "            for k, l in zip(vocab, data_features.toarray()[i]):\n",
    "                \n",
    "                #if the count of the current word is greater than 0\n",
    "                if(l>0):\n",
    "                    \n",
    "                    #if the word is present in the vocabulary\n",
    "                    if(k in vocab_stemmed):\n",
    "                        \n",
    "                        #append it to the file\n",
    "                        count_vec_file.write(str(vocab_stemmed.index(k))+\":\"+str(l)+\",\")\n",
    "            \n",
    "            #writing a new line\n",
    "            count_vec_file.write(\"\\n\")\n",
    "        \n",
    "        #closing the file\n",
    "        count_vec_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Function call and uni/bi gram file creation\n",
    "\n",
    "+ In this section, the above defined functions are called and the word documents are generated for top 100 unigram and bigram.\n",
    "\n",
    "+ Also for creating the vocab text, words per day is stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-bc890be913b5>:22: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  temp = only_text[i].encode('ascii','ignore').decode('unicode-escape')\n",
      "<ipython-input-5-bc890be913b5>:22: DeprecationWarning: invalid escape sequence '\\_'\n",
      "  temp = only_text[i].encode('ascii','ignore').decode('unicode-escape')\n",
      "<ipython-input-5-bc890be913b5>:22: DeprecationWarning: invalid escape sequence '\\l'\n",
      "  temp = only_text[i].encode('ascii','ignore').decode('unicode-escape')\n",
      "<ipython-input-5-bc890be913b5>:22: DeprecationWarning: invalid escape sequence '\\#'\n",
      "  temp = only_text[i].encode('ascii','ignore').decode('unicode-escape')\n"
     ]
    }
   ],
   "source": [
    "#Initialsing variables to store values in a key value pair\n",
    "date_dict_words ={}\n",
    "all_words = []\n",
    "day_uni_words = {}\n",
    "\n",
    "#For the range of length of the sheets\n",
    "for i in range(len(given_excel.sheet_names)):\n",
    "    \n",
    "    #Reading the excel file in each sheet per the iteration\n",
    "    per_sheet = pd.read_excel(r'31021301.xlsx',sheet_name=given_excel.sheet_names[i])\n",
    "    \n",
    "    #Removing all empty rows\n",
    "    per_sheet = per_sheet.dropna(0,how=\"all\")\n",
    "    \n",
    "    #Removing all empty column\n",
    "    per_sheet = per_sheet.dropna(1,how=\"all\")\n",
    "    \n",
    "    #Ignore the 0th index since 0th index has got the column name\n",
    "    per_sheet = per_sheet.iloc[1:]\n",
    "    \n",
    "    #Manually declaring the column name to avoid inconsistency in the data frame column name\n",
    "    per_sheet.columns=['text','id','created_at']\n",
    "    \n",
    "    #reset the index\n",
    "    per_sheet = per_sheet.reset_index(drop = True)\n",
    "    \n",
    "    #Index name is set to null\n",
    "    per_sheet.iloc[0].index.name = ''\n",
    "    \n",
    "    #Storing the contents of the column text into a list for iterating purpose\n",
    "    only_text = list(per_sheet['text'])\n",
    "    \n",
    "    #Calling the function to generate unigram tokens per day\n",
    "    tokens_gen = generate_unitoken_perday(only_text)\n",
    "    \n",
    "    #Storing the values of only english tweets\n",
    "    all_words.append(tokens_gen[0])\n",
    "    \n",
    "    #Storing the values of tokenised words per day\n",
    "    ind_uni_words = tokens_gen[1]\n",
    "    \n",
    "    #Converting to set and then to list to remove duplicates\n",
    "    unique_per_day = list(set(ind_uni_words))\n",
    "    \n",
    "    #Adding every values to a key value pair so as to generate Vocab text and count vector \n",
    "    date_dict_words[given_excel.sheet_names[i]] = unique_per_day\n",
    "    \n",
    "    #For bigram generation\n",
    "    eng_only_uni = tokens_gen[0]\n",
    "    \n",
    "    #Function call to retrive tokens\n",
    "    ind_bi_words = generate_bitoken_perday(eng_only_uni)\n",
    "    day_uni_words[given_excel.sheet_names[i]] = ind_bi_words\n",
    "    \n",
    "    #Stemming is done on the unigram words\n",
    "    vocab_stemmed = []\n",
    "    for a in ind_uni_words:\n",
    "            vocab_stemmed.append(stemmer.stem(a))\n",
    "\n",
    "    #Function call to generate top 100 unigrams for the day\n",
    "    generate_bigram(ind_bi_words,given_excel.sheet_names[i])\n",
    "    \n",
    "    #Function call to generate top 100 bigrams for the day\n",
    "    generate_unigram(vocab_stemmed,given_excel.sheet_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Generating Vocab Text\n",
    "\n",
    "+ In this section generation of Vocabulary text file is done. The process for generating the vocab file is as follows:\n",
    "\n",
    "    + Since every day's token is stored as a list from the above 2.6 section (all_words), merging the list of list to a single list which contains all english tweets alone. \n",
    "    \n",
    "    + Tokenization is done on the tweets and seperate tokens are extracted based on the given regular expression.\n",
    "    \n",
    "    + For finding top 200 bigrams, BigramCollocationFinder and PMI measure is used. \n",
    "    \n",
    "    + MWETokenizer is applied on each day's token to extract the collocated words.\n",
    "    \n",
    "    + Context dependent stop words are found based on the given criteria and removed.\n",
    "        \n",
    "           + If the words has been repeated for more than 60 days.\n",
    "           \n",
    "           + Rare tokens : If the words has been repeated for less than 5 days.\n",
    "           \n",
    "    + Context independent stop words are imported from the text file provided and removed.\n",
    "    \n",
    "    + Stemming is done using porter stemmer.\n",
    "    \n",
    "    + Generated Vocab list and written it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the list of list to a single list\n",
    "sep_tweets = [val for sublist in all_words for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function call generate_raw_tokens(sep_tweets) with the list created above to\n",
    "#get the tokenised words\n",
    "token_words = generate_raw_tokens(sep_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202318\n"
     ]
    }
   ],
   "source": [
    "#Function call generate_top200_bigrams(token_words,day_uni_words) \n",
    "#to get the collocated words\n",
    "colloc_voc = generate_top200_bigrams(token_words,day_uni_words)\n",
    "print(len(colloc_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201328\n"
     ]
    }
   ],
   "source": [
    "#From dictionary of the bi_grams (since no constraints are applied)\n",
    "#extracting all the values and storing each value as a single list\n",
    "temp_list = [item for sublist in date_dict_words.values() for item in sublist]\n",
    "\n",
    "#Function call to get the context dependent text words\n",
    "context_dependent = find_context_dependent_words(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function call to remove all the unwanted words and \n",
    "#to get the proper tokens\n",
    "proper_tokens = remove_unwanted_words(colloc_voc,context_dependent,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porter stemmer is used to stem the video. \n",
    "\n",
    "vocab_stemmed = []\n",
    "\n",
    "#for all proper tokens\n",
    "for i in proper_tokens:\n",
    "    \n",
    "    #if the token contains _ append bigrams\n",
    "    if '_' in i:\n",
    "        vocab_stemmed.append(i)            \n",
    "    \n",
    "    #If there are uni words, stem and append\n",
    "    else:\n",
    "        vocab_stemmed.append(stemmer.stem(i)) \n",
    "\n",
    "#Creating a set of unique stems\n",
    "vocab_stemmed = set(vocab_stemmed)\n",
    "\n",
    "#converting it back to list and sorting it\n",
    "vocab_stemmed = sorted(list(vocab_stemmed)) \n",
    "\n",
    "#Function call to generate bigrams\n",
    "generate_vocab(vocab_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Generate Count Vector\n",
    "\n",
    "+ In this section, count vector has been generated. \n",
    "\n",
    "+ To achieve this task, a dictionary with key as \"day\" and value as it's \"tokens\" are used. \n",
    "\n",
    "+ Process of again finding context dependent stop words, words of length less than 3, stemming, context independent words.\n",
    "\n",
    "+ With the use of package \"CountVectorizer\", count vectors are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function call to generate vector vocabulary and this function will return \n",
    "#a dictionary with proper tokens required to create a count vector document\n",
    "day_uni_words_final = generate_countVec(day_uni_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function call to generate count vector document\n",
    "generate_vecdoc(day_uni_words_final,given_excel,vocab_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 References:\n",
    "\n",
    "1. Solutions from Lab 4 Tasks \n",
    "\n",
    "2. Solutions from Lab 5 Tasks\n",
    "\n",
    "3. Python Documentation, Iter tools. Retrived from\n",
    "https://docs.python.org/2/library/itertools.html\n",
    "\n",
    "4. jonrsharpe , Appending list of lists to a single list.Retrived from,\n",
    "https://stackoverflow.com/questions/33541947/what-does-the-built-in-function-sum-do-with-sumlist/33542054"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
